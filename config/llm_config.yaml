# LLM Provider Configuration
llm:
  # Default provider: openai, anthropic, ollama
  default_provider: "openai"

  # OpenAI Configuration
  openai:
    api_key: "${OPENAI_API_KEY}"
    model: "gpt-4"
    temperature: 0.7
    max_tokens: 4000
    timeout: 60

  # Anthropic (Claude) Configuration
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    model: "claude-3-sonnet-20240229"
    temperature: 0.7
    max_tokens: 4000
    timeout: 60

  # Ollama (Local Models) Configuration
  ollama:
    base_url: "http://localhost:11434"
    model: "llama2"
    temperature: 0.7
    timeout: 120

  # Retry Configuration
  retry:
    max_attempts: 3
    backoff_factor: 2
    max_wait: 60

  # Rate Limiting
  rate_limit:
    requests_per_minute: 60
    tokens_per_minute: 90000
